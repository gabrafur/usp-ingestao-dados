{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pyspark.sql import DataFrame, SparkSession\n",
    "from abc import ABC, abstractmethod\n",
    "import pyspark.sql.functions as F\n",
    "from pyspark.sql.types import *\n",
    "import pandas as pd\n",
    "import glob\n",
    "import logging\n",
    "import boto3\n",
    "from dotenv import load_dotenv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Rename Columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_rename = {\n",
    "    \"BanksTransformation\": {\n",
    "        \"Segmento\" : \"segmento\",\n",
    "        \"CNPJ\": \"cnpj\",\n",
    "        \"Nome\": \"nome\"\n",
    "    },\n",
    "    \"EmployeesTransformation\": {\n",
    "        \"employer-website\" : \"employer_website\",\n",
    "        \"employer-headquarters\": \"employer_headquarters\",\n",
    "        \"employer-founded\": \"employer_founded\",\n",
    "        \"employer-industry\": \"employer_industry\",\n",
    "        \"employer-revenue\": \"employer_revenue\",\n",
    "        \"Geral\": \"geral\",\n",
    "        \"Cultura e valores\": \"cultura_valores\",\n",
    "        \"Diversidade e inclus�o\": \"diversidade_inclusao\",\n",
    "        \"Qualidade de vida\": \"qualidade_vida\",\n",
    "        \"Alta lideran�a\": \"alta_lideranca\",\n",
    "        \"Remunera��o e benef�cios\": \"remuneracao_beneficios\",\n",
    "        \"Oportunidades de carreira\": \"oportunidades_carreira\",\n",
    "        \"Recomendam para outras pessoas(%)\": \"percentual_recomendam_para_outras_pessoas\",\n",
    "        \"Perspectiva positiva da empresa(%)\": \"percentual_perspectiva_positiva_empresa\",\n",
    "#         \"CNPJ\": \"cnpj\",\n",
    "        \"Nome\": \"nome\",\n",
    "        \"Segmento\": \"segmento\"\n",
    "    },\n",
    "    \"ComplaintsTransformation\": {\n",
    "        \"Ano\" : \"ano\",\n",
    "        \"Trimestre\": \"trimestre\",\n",
    "        \"Categoria\": \"categoria\",\n",
    "        \"Tipo\": \"tipo\",\n",
    "        \"CNPJ IF\": \"cnpj_if\",\n",
    "        \"Institui��o financeira\": \"instituicao_financeira\",\n",
    "        \"�ndice\": \"indice\",\n",
    "        \"Quantidade de reclama��es reguladas procedentes\": \"qtd_reclamacoes_reguladas_procedentes\",\n",
    "        \"Quantidade de reclama��es reguladas - outras\": \"qtd_reclamacoes_reguladas_outras\",\n",
    "        \"Quantidade de reclama��es n�o reguladas\": \"qtd_reclamacoes_nao_reguladas\",\n",
    "        \"Quantidade total de reclama��es\": \"qtd_total_reclamacoes\",\n",
    "        \"Quantidade total de clientes - CCS e SCR\": \"qtd_total_clientes_ccs_scr\",\n",
    "        \"Quantidade de clientes - CCS\": \"qtd_clientes_ccs\",\n",
    "        \"Quantidade de clientes - SCR\": \"qtd_clientes_scr\"\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class UnsupportedFileType(Exception):\n",
    "    def __init__(self, file_type):\n",
    "        self.file_type = file_type\n",
    "        self.message = f\"File(s) of type {file_type} not supported\"\n",
    "        super().__init__(self.message)\n",
    "\n",
    "class LoadData:\n",
    "    def __init__(self, spark, file_directory: list, file_type: str, separator: str = ';', header: bool = True, encoding='utf-8'):\n",
    "        self.file_directory = file_directory\n",
    "        self.file_type = file_type\n",
    "        self.separator = separator\n",
    "        self.spark = spark\n",
    "        self.header = header\n",
    "        self.encoding = encoding\n",
    "\n",
    "    def load(self) -> DataFrame:\n",
    "        if self.file_type in ('csv', 'tsv'):\n",
    "            return \\\n",
    "                self.spark.read.options(\n",
    "                    delimiter=self.separator,\n",
    "                    header=self.header,\n",
    "                    encoding=self.encoding\n",
    "                ).csv(self.file_directory)\n",
    "        else:\n",
    "            raise UnsupportedFileType(self.file_type)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fixing Name and DataType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformData(ABC):\n",
    "    \"\"\"\n",
    "    Gathers general functions for all transformations.\n",
    "    \"\"\"\n",
    "    def format_cnpj(self, value):\n",
    "        return F.when((value == ' ') | (value == ''), value).otherwise(F.lpad(value, 8, '0'))\n",
    "\n",
    "\n",
    "    def load_column_rename_mappings(self, transformation_name):\n",
    "#         json_file_path = os.path.join(os.path.dirname(__file__), 'column_rename.json')\n",
    "        column_rename_mappings = column_rename\n",
    "        return column_rename_mappings.get(transformation_name, {})\n",
    "\n",
    "\n",
    "    def rename_columns(self, df: DataFrame, column_rename) -> DataFrame:\n",
    "        for old_name, new_name in column_rename.items():\n",
    "            df = df.withColumnRenamed(old_name, new_name)\n",
    "        return df\n",
    "\n",
    "\n",
    "    @abstractmethod\n",
    "    def transform(self) -> DataFrame:\n",
    "        pass\n",
    "\n",
    "\n",
    "class BanksTransformation(TransformData):\n",
    "    \"\"\"\n",
    "    Functions for transforming the pandas dataframe for banks.\n",
    "    \"\"\"\n",
    "    def __init__(self, df: DataFrame):\n",
    "        \"\"\"\n",
    "        Receives the dataframe.\n",
    "        \"\"\"\n",
    "        self.df = df\n",
    "        self.column_rename = self.load_column_rename_mappings('BanksTransformation')\n",
    "\n",
    "\n",
    "\n",
    "    def transform(self) -> DataFrame:\n",
    "        \"\"\"\n",
    "        Function to rename (to snake_case), format, and adjust data in the 'Segment', 'CNPJ', and 'Name' columns.\n",
    "        Returns a dataframe.\n",
    "        \"\"\"\n",
    "        transformed_df = self.rename_columns(self.df, self.column_rename)\n",
    "        transformed_df = transformed_df.withColumn(\"cnpj\", self.format_cnpj(F.col(\"cnpj\")))\n",
    "        transformed_df = transformed_df.withColumn(\"nome\", F.regexp_replace(F.col(\"nome\").cast(StringType()), ' - PRUDENCIAL', ''))\n",
    "        return transformed_df\n",
    "\n",
    "\n",
    "class EmployeesTransformation(TransformData):\n",
    "    \"\"\"\n",
    "    Functions for transforming the pandas dataframe for employees.\n",
    "    \"\"\"\n",
    "    def __init__(self, df: DataFrame):\n",
    "        \"\"\"\n",
    "        Receives the dataframe.\n",
    "        \"\"\"\n",
    "        self.df = df\n",
    "        self.column_rename = self.load_column_rename_mappings('EmployeesTransformation')\n",
    "\n",
    "\n",
    "    def transform(self) -> DataFrame:\n",
    "        \"\"\"\n",
    "        Function to rename (to snake_case), format, and change data types.\n",
    "        Returns a dataframe.\n",
    "        \"\"\"\n",
    "        transformed_df = self.rename_columns(self.df, self.column_rename)\n",
    "\n",
    "        transformed_df = transformed_df\\\n",
    "            .withColumn(\"employer_name\", F.col('employer_name').cast(StringType()))\\\n",
    "            .withColumn(\"reviews_count\", F.col('reviews_count').cast(IntegerType()))\\\n",
    "            .withColumn(\"culture_count\", F.col('culture_count').cast(IntegerType()))\\\n",
    "            .withColumn(\"salaries_count\", F.col('salaries_count').cast(IntegerType()))\\\n",
    "            .withColumn(\"benefits_count\", F.col('benefits_count').cast(IntegerType()))\\\n",
    "            .withColumn(\"employer_website\", F.col('employer_website').cast(StringType()))\\\n",
    "            .withColumn(\"employer_headquarters\", F.col('employer_headquarters').cast(StringType()))\\\n",
    "            .withColumn(\"employer_founded\", F.col('employer_founded').cast(IntegerType()))\\\n",
    "            .withColumn(\"employer_industry\", F.col('employer_industry').cast(StringType()))\\\n",
    "            .withColumn(\"employer_revenue\", F.col('employer_revenue').cast(StringType()))\\\n",
    "            .withColumn(\"url\", F.col('url').cast(StringType()))\\\n",
    "            .withColumn(\"geral\", F.col('geral').cast(DecimalType(20,2)))\\\n",
    "            .withColumn(\"cultura_valores\", F.col('cultura_valores').cast(DecimalType(20,2)))\\\n",
    "            .withColumn(\"diversidade_inclusao\", F.col('diversidade_inclusao').cast(DecimalType(20,2)))\\\n",
    "            .withColumn(\"qualidade_vida\", F.col('qualidade_vida').cast(DecimalType(20,2)))\\\n",
    "            .withColumn(\"alta_lideranca\", F.col('alta_lideranca').cast(DecimalType(20,2)))\\\n",
    "            .withColumn(\"remuneracao_beneficios\", F.col('remuneracao_beneficios').cast(DecimalType(20,2)))\\\n",
    "            .withColumn(\"oportunidades_carreira\", F.col('oportunidades_carreira').cast(DecimalType(20,2)))\\\n",
    "            .withColumn(\"percentual_recomendam_para_outras_pessoas\", F.col('percentual_recomendam_para_outras_pessoas').cast(DecimalType(20,2)))\\\n",
    "            .withColumn(\"percentual_perspectiva_positiva_empresa\", F.col('percentual_perspectiva_positiva_empresa').cast(DecimalType(20,2)))\\\n",
    "            .withColumn(\"nome\", F.col('nome').cast(StringType()))\\\n",
    "            .withColumn(\"segmento\", F.col('segmento').cast(StringType()))\\\n",
    "            .withColumn(\"match_percent\", F.col('match_percent').cast(FloatType()))\n",
    "        #             .withColumn('cnpj', self.format_cnpj(F.col(\"cnpj\")))\\\n",
    "\n",
    "\n",
    "        return transformed_df\n",
    "\n",
    "\n",
    "    def calculate_aggregates(self, df) -> DataFrame:\n",
    "        \"\"\"\n",
    "        Function to return a pandas dataframe of a pivot table grouped by the 'name' column,\n",
    "        aggregating the 'geral' and 'remuneracao_beneficios' columns by mean.\n",
    "        \"\"\"\n",
    "        aggregated_df  = df.groupby('nome').agg(\n",
    "            F.round(F.mean('geral'), 2).alias('geral'),\n",
    "            F.round(F.mean('remuneracao_beneficios'), 2).alias('remuneracao_beneficios')\n",
    "        )\n",
    "        return aggregated_df \n",
    "\n",
    "\n",
    "class ComplaintsTransformation(TransformData):\n",
    "    \"\"\"\n",
    "    Functions for transforming the pandas dataframe for complaints.\n",
    "    \"\"\"\n",
    "    def __init__(self, df: DataFrame):\n",
    "        \"\"\"\n",
    "        Receives the dataframe.\n",
    "        \"\"\"\n",
    "        self.df = df\n",
    "        self.column_rename = self.load_column_rename_mappings('ComplaintsTransformation')\n",
    "\n",
    "\n",
    "    def transform(self) -> DataFrame:\n",
    "        \"\"\"\n",
    "        Function to rename (to snake_case), format, and change data types.\n",
    "        Returns a dataframe.\n",
    "        \"\"\"\n",
    "        transformed_df = self.rename_columns(self.df, self.column_rename)\n",
    "\n",
    "        transformed_df = transformed_df\\\n",
    "            .withColumn(\"ano\", F.col('ano').cast(IntegerType()))\\\n",
    "            .withColumn('trimestre', F.col(\"trimestre\").cast(StringType()))\\\n",
    "            .withColumn('categoria', F.col(\"categoria\").cast(StringType()))\\\n",
    "            .withColumn('tipo', F.col(\"tipo\").cast(StringType()))\\\n",
    "            .withColumn(\"cnpj\", self.format_cnpj(F.col(\"cnpj_if\")))\\\n",
    "            .withColumn(\"nome\", F.regexp_replace(F.col(\"instituicao_financeira\").cast(StringType()), ' \\(conglomerado\\)', ''))\\\n",
    "            .withColumn(\"indice\", F.regexp_replace(F.col('indice').cast(StringType()), ',', '.').cast(DecimalType(20,2)))\\\n",
    "            .withColumn(\"qtd_reclamacoes_reguladas_procedentes\", F.col('qtd_reclamacoes_reguladas_procedentes').cast(DecimalType(20,2)))\\\n",
    "            .withColumn(\"qtd_reclamacoes_reguladas_outras\", F.col('qtd_reclamacoes_reguladas_outras').cast(DecimalType(20,2)))\\\n",
    "            .withColumn(\"qtd_reclamacoes_nao_reguladas\", F.col('qtd_reclamacoes_nao_reguladas').cast(DecimalType(20,2)))\\\n",
    "            .withColumn(\"qtd_total_reclamacoes\", F.col('qtd_total_reclamacoes').cast(DecimalType(20,2)))\\\n",
    "            .withColumn(\"qtd_total_clientes_ccs_scr\", F.col('qtd_total_clientes_ccs_scr').cast(DecimalType(20,2)))\\\n",
    "            .withColumn(\"qtd_clientes_ccs\", F.col('qtd_clientes_ccs').cast(DecimalType(20,2)))\\\n",
    "            .withColumn(\"qtd_clientes_scr\", F.col('qtd_clientes_scr').cast(DecimalType(20,2)))\n",
    "\n",
    "        return transformed_df\n",
    "\n",
    "\n",
    "    def calculate_aggregates(self, df) -> DataFrame:\n",
    "        \"\"\"\n",
    "        Function to return a pandas dataframe of a pivot table grouped by the 'name' column,\n",
    "        aggregating columns like 'indice', 'qtd_total_reclamacoes', and 'qtd_total_clientes_ccs_scr' by mean.\n",
    "        \"\"\"\n",
    "        aggregated_df  = df.groupby('nome').agg(\n",
    "            F.round(F.mean('indice'), 2).alias(\"indice\"),\n",
    "            F.round(F.mean('qtd_total_reclamacoes'), 2).alias(\"qtd_total_reclamacoes\"),\n",
    "            F.max('qtd_total_clientes_ccs_scr').alias(\"qtd_total_clientes_ccs_scr\")\n",
    "        )\n",
    "        return aggregated_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Starting spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "load_dotenv(r'credencials\\.env')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "A = boto3.client(\n",
    "    's3',\n",
    "    aws_access_key_id = os.getenv('aws_access_key_id'),\n",
    "    aws_secret_access_key = os.getenv('aws_secret_access_key'),\n",
    "    aws_session_token = os.getenv('aws_session_token')\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: Ignored the following versions that require a different python version: 0.9.1 Requires-Python >=2.7,<3.0; 1.0.2 Requires-Python >=3,<3.8\n",
      "ERROR: Could not find a version that satisfies the requirement awsglue-local (from versions: none)\n",
      "ERROR: No matching distribution found for awsglue-local\n"
     ]
    }
   ],
   "source": [
    "pip3 install --upgrade jupyter boto3 aws-glue-sessions   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: Ignored the following versions that require a different python version: 0.9.1 Requires-Python >=2.7,<3.0; 1.0.2 Requires-Python >=3,<3.8\n",
      "ERROR: Could not find a version that satisfies the requirement awsglue-local (from versions: none)\n",
      "ERROR: No matching distribution found for awsglue-local\n"
     ]
    }
   ],
   "source": [
    "pip install awsglue-local"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "^C\n"
     ]
    }
   ],
   "source": [
    "!jupyter notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'awsglue'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mpyspark\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mcontext\u001b[39;00m \u001b[39mimport\u001b[39;00m SparkContext\n\u001b[1;32m----> 2\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mawsglue\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mcontext\u001b[39;00m \u001b[39mimport\u001b[39;00m GlueContext\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'awsglue'"
     ]
    }
   ],
   "source": [
    "from pyspark.context import SparkContext\n",
    "from awsglue.context import GlueContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "s3 = boto3.resource('s3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "UsageError: Line magic function `%glue_version` not found.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "%glue_version 3.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Java gateway process exited before sending its port number",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m sc \u001b[39m=\u001b[39m SparkContext()\n\u001b[0;32m      2\u001b[0m glueContext \u001b[39m=\u001b[39m GlueContext(sc)\n\u001b[0;32m      3\u001b[0m spark \u001b[39m=\u001b[39m glueContext\u001b[39m.\u001b[39mspark_session\n",
      "File \u001b[1;32mc:\\GitHub\\usp-ingestao-dados\\.venv\\Lib\\site-packages\\pyspark\\context.py:198\u001b[0m, in \u001b[0;36mSparkContext.__init__\u001b[1;34m(self, master, appName, sparkHome, pyFiles, environment, batchSize, serializer, conf, gateway, jsc, profiler_cls, udf_profiler_cls, memory_profiler_cls)\u001b[0m\n\u001b[0;32m    192\u001b[0m \u001b[39mif\u001b[39;00m gateway \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m gateway\u001b[39m.\u001b[39mgateway_parameters\u001b[39m.\u001b[39mauth_token \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    193\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[0;32m    194\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mYou are trying to pass an insecure Py4j gateway to Spark. This\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    195\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39m is not allowed as it is a security risk.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    196\u001b[0m     )\n\u001b[1;32m--> 198\u001b[0m SparkContext\u001b[39m.\u001b[39;49m_ensure_initialized(\u001b[39mself\u001b[39;49m, gateway\u001b[39m=\u001b[39;49mgateway, conf\u001b[39m=\u001b[39;49mconf)\n\u001b[0;32m    199\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m    200\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_do_init(\n\u001b[0;32m    201\u001b[0m         master,\n\u001b[0;32m    202\u001b[0m         appName,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    212\u001b[0m         memory_profiler_cls,\n\u001b[0;32m    213\u001b[0m     )\n",
      "File \u001b[1;32mc:\\GitHub\\usp-ingestao-dados\\.venv\\Lib\\site-packages\\pyspark\\context.py:432\u001b[0m, in \u001b[0;36mSparkContext._ensure_initialized\u001b[1;34m(cls, instance, gateway, conf)\u001b[0m\n\u001b[0;32m    430\u001b[0m \u001b[39mwith\u001b[39;00m SparkContext\u001b[39m.\u001b[39m_lock:\n\u001b[0;32m    431\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m SparkContext\u001b[39m.\u001b[39m_gateway:\n\u001b[1;32m--> 432\u001b[0m         SparkContext\u001b[39m.\u001b[39m_gateway \u001b[39m=\u001b[39m gateway \u001b[39mor\u001b[39;00m launch_gateway(conf)\n\u001b[0;32m    433\u001b[0m         SparkContext\u001b[39m.\u001b[39m_jvm \u001b[39m=\u001b[39m SparkContext\u001b[39m.\u001b[39m_gateway\u001b[39m.\u001b[39mjvm\n\u001b[0;32m    435\u001b[0m     \u001b[39mif\u001b[39;00m instance:\n",
      "File \u001b[1;32mc:\\GitHub\\usp-ingestao-dados\\.venv\\Lib\\site-packages\\pyspark\\java_gateway.py:106\u001b[0m, in \u001b[0;36mlaunch_gateway\u001b[1;34m(conf, popen_kwargs)\u001b[0m\n\u001b[0;32m    103\u001b[0m     time\u001b[39m.\u001b[39msleep(\u001b[39m0.1\u001b[39m)\n\u001b[0;32m    105\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m os\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39misfile(conn_info_file):\n\u001b[1;32m--> 106\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mJava gateway process exited before sending its port number\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m    108\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mopen\u001b[39m(conn_info_file, \u001b[39m\"\u001b[39m\u001b[39mrb\u001b[39m\u001b[39m\"\u001b[39m) \u001b[39mas\u001b[39;00m info:\n\u001b[0;32m    109\u001b[0m     gateway_port \u001b[39m=\u001b[39m read_int(info)\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Java gateway process exited before sending its port number"
     ]
    }
   ],
   "source": [
    "sc = SparkContext()\n",
    "glueContext = GlueContext(sc)\n",
    "spark = glueContext.spark_session"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Running fixes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'spark' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\99834460\\Documents\\GitHub\\usp-ingestao-dados\\atividade04\\src\\03fixing_data.ipynb Cell 10\u001b[0m in \u001b[0;36m<cell line: 44>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/99834460/Documents/GitHub/usp-ingestao-dados/atividade04/src/03fixing_data.ipynb#W4sZmlsZQ%3D%3D?line=39'>40</a>\u001b[0m     df_complaints \u001b[39m=\u001b[39m transform_complaints\u001b[39m.\u001b[39mtransform()\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/99834460/Documents/GitHub/usp-ingestao-dados/atividade04/src/03fixing_data.ipynb#W4sZmlsZQ%3D%3D?line=41'>42</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m df_banks, df_employees, df_complaints\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/99834460/Documents/GitHub/usp-ingestao-dados/atividade04/src/03fixing_data.ipynb#W4sZmlsZQ%3D%3D?line=43'>44</a>\u001b[0m df_banks, df_employees, df_complaints \u001b[39m=\u001b[39m read_and_set_datatype()\n",
      "\u001b[1;32mc:\\Users\\99834460\\Documents\\GitHub\\usp-ingestao-dados\\atividade04\\src\\03fixing_data.ipynb Cell 10\u001b[0m in \u001b[0;36mread_and_set_datatype\u001b[1;34m()\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/99834460/Documents/GitHub/usp-ingestao-dados/atividade04/src/03fixing_data.ipynb#W4sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m logger \u001b[39m=\u001b[39m logging\u001b[39m.\u001b[39mgetLogger(\u001b[39m__name__\u001b[39m)\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/99834460/Documents/GitHub/usp-ingestao-dados/atividade04/src/03fixing_data.ipynb#W4sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m logging\u001b[39m.\u001b[39mbasicConfig(level\u001b[39m=\u001b[39mlogging\u001b[39m.\u001b[39mINFO)\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/99834460/Documents/GitHub/usp-ingestao-dados/atividade04/src/03fixing_data.ipynb#W4sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m load_banks \u001b[39m=\u001b[39m LoadData(\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/99834460/Documents/GitHub/usp-ingestao-dados/atividade04/src/03fixing_data.ipynb#W4sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m     spark\u001b[39m=\u001b[39mspark,\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/99834460/Documents/GitHub/usp-ingestao-dados/atividade04/src/03fixing_data.ipynb#W4sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m     file_directory\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39ms3://bucket-boto-gabrafur-usp/raw_files/banco/\u001b[39m\u001b[39m'\u001b[39m, \n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/99834460/Documents/GitHub/usp-ingestao-dados/atividade04/src/03fixing_data.ipynb#W4sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m     file_type\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mtsv\u001b[39m\u001b[39m'\u001b[39m,\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/99834460/Documents/GitHub/usp-ingestao-dados/atividade04/src/03fixing_data.ipynb#W4sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m     separator\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39m\\t\u001b[39;00m\u001b[39m'\u001b[39m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/99834460/Documents/GitHub/usp-ingestao-dados/atividade04/src/03fixing_data.ipynb#W4sZmlsZQ%3D%3D?line=13'>14</a>\u001b[0m )\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/99834460/Documents/GitHub/usp-ingestao-dados/atividade04/src/03fixing_data.ipynb#W4sZmlsZQ%3D%3D?line=14'>15</a>\u001b[0m df_raw_banks \u001b[39m=\u001b[39m load_banks\u001b[39m.\u001b[39mload()\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/99834460/Documents/GitHub/usp-ingestao-dados/atividade04/src/03fixing_data.ipynb#W4sZmlsZQ%3D%3D?line=16'>17</a>\u001b[0m load_employees \u001b[39m=\u001b[39m LoadData(\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/99834460/Documents/GitHub/usp-ingestao-dados/atividade04/src/03fixing_data.ipynb#W4sZmlsZQ%3D%3D?line=17'>18</a>\u001b[0m     spark\u001b[39m=\u001b[39mspark,\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/99834460/Documents/GitHub/usp-ingestao-dados/atividade04/src/03fixing_data.ipynb#W4sZmlsZQ%3D%3D?line=18'>19</a>\u001b[0m     file_directory\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39ms3://bucket-boto-gabrafur-usp/raw_files/empregados/\u001b[39m\u001b[39m'\u001b[39m, \n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/99834460/Documents/GitHub/usp-ingestao-dados/atividade04/src/03fixing_data.ipynb#W4sZmlsZQ%3D%3D?line=19'>20</a>\u001b[0m     file_type\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mcsv\u001b[39m\u001b[39m'\u001b[39m,\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/99834460/Documents/GitHub/usp-ingestao-dados/atividade04/src/03fixing_data.ipynb#W4sZmlsZQ%3D%3D?line=20'>21</a>\u001b[0m     separator\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39m|\u001b[39m\u001b[39m'\u001b[39m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/99834460/Documents/GitHub/usp-ingestao-dados/atividade04/src/03fixing_data.ipynb#W4sZmlsZQ%3D%3D?line=21'>22</a>\u001b[0m )\n",
      "\u001b[1;31mNameError\u001b[0m: name 'spark' is not defined"
     ]
    }
   ],
   "source": [
    "def read_and_set_datatype() -> tuple:\n",
    "    \"\"\"\n",
    "    Initiates the main execution where a logger is initialized, the current directory is captured, and various tables are loaded,\n",
    "    transformed, concatenated into a single dataframe via join, and finally saved in a directory in the 'parquet' fixed format.\n",
    "    \"\"\"\n",
    "    logger = logging.getLogger(__name__)\n",
    "    logging.basicConfig(level=logging.INFO)\n",
    "\n",
    "    load_banks = LoadData(\n",
    "        spark=spark,\n",
    "        file_directory='s3://bucket-boto-gabrafur-usp/raw_files/banco/', \n",
    "        file_type='tsv',\n",
    "        separator='\\t'\n",
    "    )\n",
    "    df_raw_banks = load_banks.load()\n",
    "\n",
    "    load_employees = LoadData(\n",
    "        spark=spark,\n",
    "        file_directory='s3://bucket-boto-gabrafur-usp/raw_files/empregados/', \n",
    "        file_type='csv',\n",
    "        separator='|'\n",
    "    )\n",
    "    df_raw_employees = load_employees.load()\n",
    "\n",
    "    load_complaints = LoadData(\n",
    "        spark=spark,\n",
    "        file_directory='s3://bucket-boto-gabrafur-usp/raw_files/reclamacoes/', \n",
    "        file_type='csv',\n",
    "        separator=';'\n",
    "    )\n",
    "    df_raw_complaints = load_complaints.load()\n",
    "\n",
    "    transform_banks = BanksTransformation(df_raw_banks)\n",
    "    df_banks = transform_banks.transform()\n",
    "\n",
    "    transform_employees = EmployeesTransformation(df_raw_employees)\n",
    "    df_employees = transform_employees.transform()\n",
    "\n",
    "    transform_complaints = ComplaintsTransformation(df_raw_complaints)\n",
    "    df_complaints = transform_complaints.transform()\n",
    "\n",
    "    return df_banks, df_employees, df_complaints\n",
    "\n",
    "df_banks, df_employees, df_complaints = read_and_set_datatype()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "   \n",
    "    df_grouped_employees = transform_employees.calculate_aggregates(df_employees)\n",
    "    \n",
    "    df_grouped_complaints = transform_complaints.calculate_aggregates(df_complaints)\n",
    "   \n",
    "    df_complaints_banks = df_banks.join(df_grouped_complaints, on=['nome'], how='inner')\n",
    "    logging.info(f'Count Reclama��es x Bancos: \\n{df_complaints_banks.count()}')\n",
    "\n",
    "    df_complaints_banks_employees = df_complaints_banks.join(df_grouped_employees, on=['nome'], how='inner')\n",
    "    logging.info(f'Count Reclama��es x Bancos x Empregados: \\n{df_complaints_banks_employees.count()}')\n",
    "\n",
    "    output_directory = 's3://805766217211-transformed/atividade03'\n",
    "    write_data = DataWriter()\n",
    "    write_data.write_parquet(df_complaints_banks_employees, output_directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataWriter:\n",
    "    def write_parquet(self, df, output_directory, mode=\"overwrite\", repartition=True):\n",
    "        if repartition:\n",
    "            df.repartition(1).write.mode(mode).parquet(output_directory)\n",
    "        else:\n",
    "            df.write.mode(mode).parquet(output_directory)\n",
    "            \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
